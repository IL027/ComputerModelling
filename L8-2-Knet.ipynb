{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    " * AutoGrad\n",
    " * Logistic Regression revisited\n",
    " * Multiclass logistic regression\n",
    " * Multilayer Neural Networks and Deeplearning\n",
    " \n",
    "closely following [Knet-the-Julia-dope](https://github.com/moralesq/Knet-the-Julia-dope) in turn based on [Deep Learning - The Straight Dope](http://gluon.mxnet.io/) using MXNet which can also be accessed from Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Introduction\n",
    "Typically gradients in neural network are computed using back-propagation. An alternative is to use autograd.  This is the approach taken by the popular [autograd](https://github.com/HIPS/autograd) Python package and its Julia port AutoGrad.jl used by Knet.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AutoGrad\n",
    "using Knet, Plots, DataFrames\n",
    "gr()\n",
    "using Suppressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a toy example:  $f = 2x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x    = [1 2; 3 4];\n",
    "f(x) = 2x^2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Knet g=grad(f) generates a gradient function g, which takes the same inputs as the function f but returns the gradient. The gradient function g triggers recording by boxing the parameters in a special data type and calls f. The elementary operations in f are overloaded to record their actions and output boxed answers when their inputs are boxed. The sequence of recorded operations is then used to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad(f)\n",
    "g(5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# (Stochastic) Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize $f$ we consider the recursion:\n",
    "$$ x_{n+1}=x_{n}-\\epsilon \\nabla f(x_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "xs=Float64[]\n",
    "lr=0.05\n",
    "x=4.0\n",
    "for i=1:100\n",
    "    x-=lr *g(x)\n",
    "    push!(xs,x)\n",
    "end\n",
    "plot([1:100;],xs); ylabel!(\"x\");xlabel!(\"steps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# (Stochastic) Gradient Descent\n",
    "In order to minimize $f$ we consider the recursion:\n",
    "$$ x_{n+1}=x_{n}-\\epsilon \\widehat{\\nabla f}_n(x_n)$$\n",
    "where  $\\mathbb{E}\\widehat{\\nabla f}_n(x)=\\nabla f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "xs=Float64[]\n",
    "lr=0.05\n",
    "x=4.0\n",
    "for i=1:100\n",
    "    x-=lr *(g(x)+randn())\n",
    "    push!(xs,x)\n",
    "end\n",
    "plot([1:100;],xs)\n",
    "ylabel!(\"x\");xlabel!(\"steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary classification with logistic regression\n",
    "\n",
    "\n",
    "The simplest kind of classification problem is *binary classification*, when there are only two categories, so let's start there. Let's call our two categories the positive class $y_i=1$ and the negative class $y_i = 0$ (another common way of defining the labels are $y_i=\\pm1$). Even with just two categories, and even confining ourselves to linear models, \n",
    "there are many ways we might approach the problem. For example, we might try to draw a line that best separates the points:\n",
    "\n",
    "![](../img/linear-separator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that in linear regression, we made predictions of the form\n",
    "\n",
    "$$ \\hat{y} = \\boldsymbol{w}^T \\boldsymbol{x} + b, $$\n",
    "\n",
    "where $\\hat{y},b\\in\\mathbb{R}$ and $\\boldsymbol{w},\\boldsymbol{x}\\in\\mathbb{R}^d$. We are interested in asking the question *\"what is the probability that example $\\boldsymbol{x}$ belongs to the positive class?\"* A regular linear model is a poor choice here because it can output values greater than $1$ or less than $0$. To coerce reasonable answers from our model,  we're going to modify it slightly, by running the linear function through a sigmoid activation function $\\sigma$:\n",
    "\n",
    "$$ \\hat{y} =\\sigma(\\boldsymbol{w}^T \\boldsymbol{x} + b). $$\n",
    "\n",
    "The sigmoid function $\\sigma$, sometimes called a squashing function or a *logistic* function - t- maps a real-valued input to the range 0 to 1. If we pick the labels $y\\in(0,1)$ we may assign  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(y=1|z) & =\\sigma(z)=\\frac{1}{1+e^{-z}}\\\\\n",
    "\\mathbb{P}(y=0|z) & =1-\\sigma(z)=\\frac{1}{1+e^{z}}\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Compact form: $\\mathbb{P}(y|z)  =\\sigma(z)^y(1-\\sigma(z))^{1-y}$. Let us define and visualize this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic function\n",
    "sigmoid(z) = 1 ./ (1 + exp.(-z))\n",
    "plot(-5:0.1:5, sigmoid(-5:0.1:5), xlabel=:z, ylabel=\"sigmoid(z)\", title=\"Logistic Function\", legend=false, size=(400,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fitting the model\n",
    "\n",
    "Maximum likelihood estimator\n",
    "$$\\max_{\\theta} \\mathbb{P}_{\\theta}\\big( y_1,\\dots,y_n \\big|\\,\\boldsymbol{x}_1,\\dots\\boldsymbol{x}_n \\big)=\\max_\\theta \\prod_i^n\\mathbb{P}_\\theta(y_i| \\boldsymbol{x}_i)$$\n",
    "\n",
    "Because each example is independent of the others.\n",
    "\n",
    "\n",
    "$$\\max_\\theta \\log\\big(\\prod_i^n\\mathbb{P}(y_i|\\boldsymbol{x}_i)\\big)= \\sum_i^m\\log\\big(\\mathbb{P}(y_i|\\boldsymbol{x}_i)\\big)=\\log\\big(\\mathbb{P}(y_1|\\boldsymbol{x}_1)\\big)+\\cdots+\\log\\big(\\mathbb{P}(y_n|\\boldsymbol{x}_n)\\big)$$\n",
    "\n",
    "Because we typically express our objective as a *loss* we can just flip the sign, giving us the *negative log probability:*\n",
    "\n",
    "$$  \\min_\\theta \\Big(- \\sum_i^m\\log\\big(\\mathbb{P}(y_i|\\boldsymbol{x}_i)\\big)\\Big)$$\n",
    "\n",
    "We can write $\\mathbb{P}_\\theta(y_1|z_i)$ compactly as\n",
    "\n",
    "$$\\mathbb{P}_\\theta(y_i|z_i) =\\sigma(z_i)^{y_i}(1-\\sigma(z_i))^{1-y_i},$$\n",
    "\n",
    "$$\n",
    "\\log\\big(\\mathbb{P}_\\theta(y|z)\\big)=yz + \\log\\sigma(-z)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Note that this loss function is commonly called *log loss* and also commonly referred to as *binary cross entropy*. It is a special case of negative log [likelihood](https://en.wikipedia.org/wiki/Likelihood_function). And it is a special case of [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy), which can apply to the multi-class ($>2$) setting. \n",
    "\n",
    "**If instead we were to use the labels $y_i=\\pm1$, the loss function has to modified to $\\log(1+e^{-z})$. This usually leads to a lot of confussion as to why there exists two versions of logistic regression. See [here](https://stats.stackexchange.com/questions/250937/which-loss-function-is-correct-for-logistic-regression/279698#279698) for more information on the topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Adult Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Adult dataset taken from the [UCI repository](http://archive.ics.uci.edu/ml/datasets.html). \n",
    "\n",
    " * the dataset contained $14$ features, including age, education, occupation, sex, native-country, among others. In this version, hosted by [National Taiwan University](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html),\n",
    " * The label is a binary indicator indicating whether the person corresponding to each row made more ($y_i = 1$) or less ($y_i = 0$) than $50,000 of income in 1994. \n",
    " * The dataset we're working with contains 30,956 training examples and 1,605 examples set aside for testing. We can download and read the datasets into main memory like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "url_train = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a2a.t\"\n",
    "url_test  = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a\"\n",
    "\n",
    "if !isfile(\"adult.train\")\n",
    "    rawdata_train = readtable(download(url_train, \"adult.train\"), header=false);\n",
    "else\n",
    "    rawdata_train = readtable(\"adult.train\", header=false);\n",
    "end\n",
    "\n",
    "if !isfile(\"adult.test\")\n",
    "    rawdata_test  = readtable(download(url_test, \"adult.test\"), header=false);\n",
    "else\n",
    "    rawdata_test  = readtable(\"adult.test\", header=false);    \n",
    "end\n",
    "\n",
    "@sprintf \"Training size = %d    Testing size = %d\" size(rawdata_train, 1) size(rawdata_test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data consists of lines like the following:\n",
    "\n",
    "-1 4:1 6:1 15:1 21:1 35:1 40:1 57:1 63:1 67:1 73:1 74:1 77:1 80:1 83:1\n",
    "\n",
    "The first entry in each row is the value of the label. The following tokens are the indices of the non-zero features. The number $1$ here is redundant. But we don't always have control over where our data comes from, so we might as well get used to mucking around with weird file formats. Let's write a simple script to process our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nonzeroindex(sample)\n",
    "\n",
    "    s      = split(sample, \":1 \") \n",
    "    val    = parse(Int64, split(sample)[1])\n",
    "    s[1]   = split(s[1])[2]\n",
    "    s[end] = split(s[end], \":\")[1]\n",
    "    \n",
    "    output = zeros(Float32, 1, 124)\n",
    "    output[parse.(Int64, s)] = 1\n",
    "    output[end] = val\n",
    "    return output\n",
    "end\n",
    "\n",
    "function processdata(rawdata; atype=Array{Float32})\n",
    "    data = map(atype, [vcat(nonzeroindex.(rawdata[1])...)'])[1];\n",
    "    # change label from {-1,1} to {0,1}\n",
    "    x, y = map(atype, [data[1:end-1, :], (data[end:end, :] + 1) / 2])\n",
    "    return x, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "atype=Array{Float32}; \n",
    "xtrn, ytrn  = processdata(rawdata_train, atype=atype);\n",
    "xtst, ytst  = processdata(rawdata_test, atype=atype);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the fraction of positive examples in our training and test sets. This will give us one nice (necessay but insufficient) sanity check that our training and test data really are drawn from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ytrn) / length(ytrn), sum(ytst) / length(ytst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Knet's mini batch function sets up the stochastic gradient \n",
    "$$\\nabla \\sum_i^m\\log\\big(\\mathbb{P}(y_i|\\boldsymbol{x}_i)\\big) \\approx \\frac{m}{s} \\sum_i^s \\nabla \\log\\big(\\mathbb{P}(y_{\\tau_s}|\\boldsymbol{x}_{\\tau_s})\\big)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here $\\{\\tau_1,\\dots,\\tau_s\\}$ is a random subset of  $\\{1,\\dots,m\\}$. $s$ is called the batch sizes and stochastic gradients are implemented through Knets *minibatch* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrn = Knet.minibatch(xtrn, ytrn, 64, shuffle=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the loss\n",
    "*w* here are the weights which are choosen to minimise the cross entropy loss. This achieved by writing the loss and taking the gradient with respect the first variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(w, x) = w[1] * x .+ w[2];\n",
    "\n",
    "function loss(w, x, y)\n",
    "    yhat = sigm.(pred(w, x))\n",
    "    return -sum(y .* log.(yhat) + (1-y) .* log.(1-yhat))\n",
    "end\n",
    "lossgradient  = grad(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The training loop consists of an \n",
    "  * outerloop counting the epochs (effective iterations through the data set).\n",
    "  * an inner loop that das gradient descent based on the stochastic gradient of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "function train(w, dtrn; lr=1e-6, epochs=5)\n",
    "    tloss = []\n",
    "    for epoch = 1:epochs\n",
    "        eloss = 0\n",
    "        for (x,y) in dtrn\n",
    "            eloss += loss(w, x, y)\n",
    "            g = lossgradient(w, x, y)\n",
    "            for i = 1:length(w)\n",
    "                w[i] -= lr * g[i]\n",
    "            end\n",
    "        end\n",
    "        push!(tloss, eloss/length(dtrn))\n",
    "    end\n",
    "    \n",
    "    return w, tloss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training output\n",
    "Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = map(atype, Any[randn(1, size(xtrn, 1)), zeros(Float32,1,1) ]);\n",
    "Accuracy(w, xtst, ytst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, Loss = train(w, dtrn; epochs=30, lr=1e-2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show Accuracy(w, xtrn, ytrn)\n",
    "@show Accuracy(w, xtst, ytst)@show "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interpretation of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " * A naive classifier would predict that nobody had an income greater than $50k (the majority class). achieve an accuracy of roughly 75\\%. \n",
    " * By contrast, our classifier gets an accuracy of .84 (results may vary a small amount on each run owing to random initializations and random sampling of the batches).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiclass Logistic Regression \n",
    "\n",
    "\n",
    "* Binary classification is quite useful. (spam vs. not spam or cancer vs not cancer)\n",
    "* $k$ classes  (different handwritten digits).  What can we do?\n",
    " 1. use binary classifier in clever way \n",
    "   * train $K$ different binary classifiers $f_k(\\boldsymbol{x})$ use the one with highest probability\n",
    "   * repeated one vs the rest \n",
    " 2. generate an output that is  a discrete probability distribution over the $K$ classes. \n",
    " \n",
    "This *softmax* function achieves this by \n",
    "\n",
    "$$\\mbox{softmax}(\\boldsymbol{z}) = \\frac{e^{\\boldsymbol{z}} }{\\sum_{k=1}^K e^{z_i}},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because now we have $K$ outputs - we can represent this graphically.\n",
    "![](../img/simple-softmax-net.png)\n",
    "\n",
    " mapping from inputs to outputs via a matrix-vector product $W \\boldsymbol{x} + \\boldsymbol{b}$:\n",
    "\n",
    "$$\\hat{y} = \\mbox{softmax}(W\\boldsymbol{x} + \\boldsymbol{b})$$\n",
    "\n",
    "This model is sometimes called *multiclass logistic regression*, *softmax regression* and *multinomial regression*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "In this example we build a simple classification model for the [MNIST](http://yann.lecun.com/exdb/mnist/) handwritten digit recognition dataset. MNIST has 60000 training and 10000 test examples. Each input x consists of 784 pixels representing a 28x28 image. The corresponding output indicates the identity of the digit 0..9.\n",
    "\n",
    "![png](https://jamesmccaffrey.files.wordpress.com/2014/06/firsteightimages.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To start, we'll use Knet's utilities for grabbing a copy of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for p in (\"GZip\",)\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end\n",
    "\n",
    "using GZip\n",
    "\n",
    "\"Where to download mnist from\"\n",
    "mnisturl = \"http://yann.lecun.com/exdb/mnist\"\n",
    "\n",
    "\"Where to download mnist to\"\n",
    "mnistdir = \"./\"\n",
    "\n",
    "\"\"\"\n",
    "This utility loads the [MNIST](http://yann.lecun.com/exdb/mnist)\n",
    "hand-written digits dataset.  There are 60000 training and 10000 test\n",
    "examples. Each input x consists of 784 pixels representing a 28x28\n",
    "grayscale image.  The pixel values are converted to Float32 and\n",
    "normalized to [0,1].  Each output y is a UInt8 indicating the correct\n",
    "class.  10 is used to represent the digit 0.\n",
    "```\n",
    "# Usage:\n",
    "include(Pkg.dir(\"Knet/data/mnist.jl\"))\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "# xtrn: 28×28×1×60000 Array{Float32,4}\n",
    "# ytrn: 60000-element Array{UInt8,1}\n",
    "# xtst: 28×28×1×10000 Array{Float32,4}\n",
    "# ytst: 10000-element Array{UInt8,1}\n",
    "```\n",
    "\"\"\"\n",
    "function mnist()\n",
    "    global _mnist_xtrn,_mnist_ytrn,_mnist_xtst,_mnist_ytst\n",
    "    if !isdefined(:_mnist_xtrn)\n",
    "        info(\"Loading MNIST...\")\n",
    "        _mnist_xtrn = _mnist_xdata(\"train-images-idx3-ubyte.gz\")\n",
    "        _mnist_xtst = _mnist_xdata(\"t10k-images-idx3-ubyte.gz\")\n",
    "        _mnist_ytrn = _mnist_ydata(\"train-labels-idx1-ubyte.gz\")\n",
    "        _mnist_ytst = _mnist_ydata(\"t10k-labels-idx1-ubyte.gz\")\n",
    "    end\n",
    "    return _mnist_xtrn,_mnist_ytrn,_mnist_xtst,_mnist_ytst\n",
    "end\n",
    "\n",
    "\"Utility to view a MNIST image, requires the Images package\"\n",
    "mnistview(x,i)=colorview(Gray,permutedims(x[:,:,1,i],(2,1)))\n",
    "\n",
    "function _mnist_xdata(file)\n",
    "    a = _mnist_gzload(file)[17:end]\n",
    "    reshape(a ./ 255f0, (28,28,1,div(length(a),784)))\n",
    "end\n",
    "\n",
    "function _mnist_ydata(file)\n",
    "    a = _mnist_gzload(file)[9:end]\n",
    "    a[a.==0] = 10\n",
    "    # full(sparse(a,1:length(a),1f0,10,length(a)))\n",
    "    return a\n",
    "end\n",
    "\n",
    "function _mnist_gzload(file)\n",
    "    if !isdir(mnistdir)\n",
    "        mkpath(mnistdir)\n",
    "    end\n",
    "    path = joinpath(mnistdir,file)\n",
    "    if !isfile(path)\n",
    "        url = \"$mnisturl/$file\"\n",
    "        download(url, path)\n",
    "    end\n",
    "    f = gzopen(path)\n",
    "    a = read(f)\n",
    "    close(f)\n",
    "    return(a)\n",
    "end\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two parts of the dataset for training and testing. Each part has N items and each item is a tuple of an image and a label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "\n",
    "size(xtrn), size(ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each image has been formatted as a 3-tuple (height, width, channel). For color images, the channel would have 3 dimensions (red, green and blue). In this case we have a gray scale image with a single channel. Note that `ytrn` is a `Array{UInt8,1}` data type. Let us take a look at these labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrn[1:10], 1ytrn[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ok, let us now use Knet's minibatch function to prepare the date for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrn = minibatch(xtrn, ytrn, 100; xtype=atype);\n",
    "dtst = minibatch(xtst, ytst, 100; xtype=atype);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input images have been reshaped to 784x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrn.xtype, dtrn.ytype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Multiclass logistic regression\n",
    "\n",
    "### Multiclass logistic regression\n",
    "\n",
    "Given $W$ and $b$ what is the loss?\n",
    "\n",
    " 1. The misclassification loss function: counts the number of instances inaccurately classified\n",
    " 2. Similar to logistic regression we would like to take gradients to find $W_{opt}$ and $b_{opt}$\n",
    "\n",
    "\n",
    "The relevant loss function here is called [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy). The cross entropy between two probability distributions $p$ and $q$ is defined as\n",
    "\n",
    "$$H(p,q)=\\operatorname {E}_{p}[-\\log q]=H(p)+D_{{{\\mathrm  {KL}}}}(p\\|q)=-\\sum_{i=1}^n p_i \\log q_i$$\n",
    "\n",
    "We typically observe fixed labels corresponding to $p_{i}=\\begin{cases}\n",
    "1 & i=k\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$.\n",
    "\n",
    "Thus the loss corresponds to the log loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "Assument that $y$ is an $K\\times m$ matrix, where $m$ is the number of samples and $K$ is the number of labels. A Naive implementaion of this theory is a follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(z) = exp.(z) ./ sum(exp.(z), 1)\n",
    "cross_entropy(yhat, y) = - sum(y .* log.(yhat), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000;\n",
    "K = 10;\n",
    "z = rand(K, m);\n",
    "yhat = softmax(z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that indeed all of our rows sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapslices(sum, yhat, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We could then implement the loss functions as: (in the code of Knet a more numerically stable version is implemented as nll )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(yhat, y) = - sum(sum(z .* log.(z), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent for multiclass logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(w,x) = w[1]*mat(x) .+ w[2] # mat takes 28,28,1,N) x array to a (784,N).\n",
    "loss(w,x,ygold) = nll(predict(w,x), ygold)\n",
    "lossgradient = grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a model on the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(w, data; lr=.1)\n",
    "    for (x,y) in data\n",
    "        dw = lossgradient(w, x, y)\n",
    "        for i in 1:length(w)\n",
    "            w[i] -= lr * dw[i]\n",
    "        end   \n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "w = map(atype, Any[ 0.1f0*randn(Float32, 10, 784), zeros(Float32, 10, 1) ])\n",
    "w = Any[ 0.1f0*randn(Float32,10,784), zeros(Float32,10,1) ];\n",
    "println((:epoch, 0, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))\n",
    "for epoch=1:10\n",
    "    train(w, dtrn; lr=0.5)\n",
    "    println((:epoch, epoch, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ok, let us check the accuracy on the testing and training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(w, dtrn, predict), accuracy(w, dtst, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might reasonably conclude that this problem is too easy to be taken seriously by experts.\n",
    "But until recently, many papers (Google Scholar says 13,800) were published using results obtained on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multilayer perceptrons from scratch\n",
    "\n",
    "Multiclass logistic regression based on $$\\hat{y} = \\mbox{softmax}(W \\boldsymbol{x} + b)$$to  **deep neural networks** with multiple layers.\n",
    "\n",
    "\n",
    "Graphically, we could depict the model like this:\n",
    "![](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/img/simple-softmax-net.png?raw=true)\n",
    "\n",
    "\n",
    " * *But linearity is a strong assumption*. - each pixel,  increasing its value either increases probability that it depicts a dog or decreases it.\n",
    "\n",
    "We can model a more general class of functions by incorporating one or more *hidden layers*.\n",
    "Each layer feeds in to the layer above it, until we generate an output.\n",
    "This architecture is commonly called a **\"multilayer perceptron\"**.\n",
    "\n",
    "$$ h_1 = \\phi(W_1\\boldsymbol{x} + b_1) $$\n",
    "$$ h_2 = \\phi(W_2\\boldsymbol{h_1} + b_2) $$\n",
    "$$...$$\n",
    "$$ h_n = \\phi(W_n\\boldsymbol{h_{n-1}} + b_n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " * each layer has its own parameters $W_i$ and $b_i$  \n",
    " * its output is feed throughactivation function for the hidden layers as $\\phi$\n",
    " * topmost hidden layer,  for classification, we'll stick with the softmax activation in the output layer.\n",
    "\n",
    "$$ \\hat{y} = \\mbox{softmax}(W_y \\boldsymbol{h}_n + b_y)$$\n",
    "\n",
    "Graphically, a multilayer perceptron could be depicted like this:\n",
    "\n",
    "![](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/img/multilayer-perceptron.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Activation energy\n",
    "function leaky_relu(x, alpha=0.2)\n",
    "    pos = max(0,x)\n",
    "    neg = min(0,x) * alpha\n",
    "    return pos + neg\n",
    "end\n",
    "xs=[-5.0:0.1,5;]\n",
    "plot(xs,relu.(xs))\n",
    "plot!(xs,leaky_relu.(xs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Remark\n",
    " * It's easy to design a hidden node that that do arbitrary computation,\n",
    "say logical operations.\n",
    " * Any function can be approximated by single-hidden-layer neural network\n",
    " * Actually learning the weights that function is the hard part. (might be easier for deeper networks)\n",
    " * Choice of architecture is tricky and a craft\n",
    "  * Fully connected layer has many parameters - special purpose convolutional layers \n",
    "  * inspired from our brain - e.g. visual cortex\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "dtrn = minibatch(xtrn, ytrn, 100, xtype=atype);\n",
    "dtst = minibatch(xtst, ytst, 100, xtype=atype);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "function initweights(d, scale=0.01; hidden=[2], atype=Array{Float32})\n",
    "    model = Vector{Any}(2 * length(hidden))\n",
    "    X = d\n",
    "    for k = 1:length(hidden)\n",
    "        H = hidden[k]\n",
    "        model[2k - 1] = scale * randn(H, X) \n",
    "        model[2k]     = scale * randn(H, 1)\n",
    "        X = H\n",
    "    end\n",
    "    return map(atype, model)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can define the function `initmodel` with the desired parameters. The variable `hidden` contains the output sizes for each of the layers, and `num_inputs` is the size of the input variable `x` (in this case $x\\in\\mathbb{R}^{784}$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initmodel(atype;num_inputs=784,num_hidden=256,num_outputs=10)\n",
    "    return initweights(num_inputs,hidden=[num_hidden,num_hidden,num_outputs]; atype=atype);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function predict(w, x)\n",
    "    x = mat(x)\n",
    "    for i=1:2:length(w) - 2\n",
    "        x = relu.(w[i] * x .+ w[i+1]) # bias an weights are concatendated \n",
    "    end\n",
    "    return w[end - 1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's test the predict function to make sure everything works fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for (x, y) in dtrn\n",
    "    display(predict(initmodel(atype), x))\n",
    "    break \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss(w, x, ygold, predict) = nll(predict(w, x), ygold);\n",
    "lossgradient = grad(loss); # AutoGrad means we don't need backpropagation\n",
    "\n",
    "function train(w, dtrn, optim, predict; epochs=10)\n",
    "    for epoch = 1:epochs\n",
    "        for (x, y) in dtrn\n",
    "            g = lossgradient(w, x, y, predict)\n",
    "            update!(w, g, optim) ## this a generic train loop the gradient update can be replaced as appropriate\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimizer (SGD) and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim(w; lr=0.01) = optimizers(w, Sgd;  lr=lr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function report(epoch, w, dtrn, dtst, predict)\n",
    "    println((:epoch, epoch, :trn, accuracy(w, dtrn, predict), :tst, accuracy(w, dtst, predict)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "w   = initmodel(atype);\n",
    "opt = optim(w, lr=1e-1);\n",
    "fast=true\n",
    "nepochs=2\n",
    "if fast\n",
    "    train(w, dtrn, opt, predict; epochs=nepochs)\n",
    "else\n",
    "    for epoch = 1:nepochs\n",
    "        train(w, dtrn, opt, predict, epochs=1)\n",
    "        report(epoch, w, dtrn, dtst, predict)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond basic Neural Nets and tricks of the trade\n",
    "\n",
    " * \"better\" optimisers, see a nice [blog post](http://sebastianruder.com/optimizing-gradient-descent/)\n",
    "  * Adam reduce noise by taking geometric average of gradients, and scale by componentwise standarddeviation estimate\n",
    "  * \"train in parallel\" - elastic averaged stochastic gradient\n",
    " * initialisation of neural nets Xavier (variance inverse proportional to number of connections of neurons)\n",
    " * regularisation overfitting - perturbing with noise the input (slight change to image should not change label), randomly disable neurons (dropout)\n",
    " * Transfer learning incorporate neural network trained on one data set for another task\n",
    "  * use as part of architecture and online retrain new components\n",
    "  * the neural network based classifing cats and dogs can help to detect skin cancer in this [Nature publication ](https://www.nature.com/articles/nature21056?error=cookies_not_supported&code=fab53529-b08e-48c2-a26a-72c23e3d69e9)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
