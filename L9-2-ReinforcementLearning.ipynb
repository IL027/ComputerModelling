{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Â IL027 Interdisciplinary Computer Modelling\n",
    "\n",
    "## Lecture 9 (Part 2) - Introduction to ANNs in Reinforcement Learning \n",
    "\n",
    "### Berk Onat \n",
    "#### Warwick Predictive Modelling Centre, School of Engineering\n",
    "##### Email: B.Onat@warwick.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement Learning \n",
    "\n",
    "#### Reinforcement learning is a sub-field of Machine Learning that focuses on how an <span style=\"color:red\">agent</span> learn to maximize the <span style=\"color:green\">reward</span> by applying <span style=\"color:orange\">actions</span> for a <span style=\"color:blue\">state</span> in an `environment`.\n",
    "\n",
    "<center><img src=\"imgL9/reinforce-agent.png\" style=\"center\" width=30%/></center>\n",
    "\n",
    "(image reference: wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement Learning \n",
    "Reinforcement learning is based on Markov Decision Process (MDP). In Markov Decision Process, any future event is independent of the past events where the only event considered is present. \n",
    "\n",
    "MDP refers to `sthocastic model` where probability of next events are solely depend on the present state. \n",
    "\n",
    "In reinforcement learning, the decision process based on the probability distribution of states is considered to be determined with a `policy` where an `agent` can take `actions` using this `policy`. \n",
    "\n",
    "Here in this Lecture, we will exercise if the policy can be determined with an ANN. \n",
    "\n",
    "We will train an ANN model to act as an AI that takes actions in an environment according to the state it is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In reinforcement learning, the Markov decision process (MDP) can be defined with ($S$, $A$, $P$, $R$) tuples as following:\n",
    "\n",
    "$S$ : a finite set of `states` \n",
    "\n",
    "$A$ : a finite set of `actions`\n",
    "\n",
    "$P(s,s') = Pr(s_{t+1} =s' | s=s_t,a=a_t)$ : the propability to transition from `s` to `s'` with action `a`.\n",
    "\n",
    "$R(s.s')$ : the reward for the transition from $s$ to $s'$.\n",
    "\n",
    "Here, `t` is the time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can we use Reinforcement Learning and train an `Agent` (AI) to play ball ?\n",
    "\n",
    "|![A](imgL9/RoboKid_Play_Ball.png)|![B](imgL9/reinforce-agent.png)|\n",
    "|:---:|:---:|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We can see an example AI-agent (<span style=\"color:pink\">pink ball</span>) that plays with <span style=\"color:purple\">purple ball</span> in animation at left hand side. \n",
    "|![A](imgL9/AI-plays-ball.gif)|![B](imgL9/reinforce-agent.png)|\n",
    "|:---:|:---:|\n",
    "\n",
    "As in the example above, we will keep the environment very simple. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Colors, Combinatorics\n",
    "using Plots\n",
    "using FileIO\n",
    "using BSON: @save, @load\n",
    "using Printf\n",
    "gr(fmt=:png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### To keep the positions, velocities and other properties of balls, we define a struct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Ball\n",
    "    id::Int\n",
    "    pos::Vector{Float64}\n",
    "    vel::Vector{Float64}\n",
    "    color::Symbol\n",
    "    r::Float64\n",
    "    m::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Here we define a function for a circle centred at (x,y) position with radius `r` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function circle(x,y,r)\n",
    "    theta = LinRange(0, 2*pi, 100)\n",
    "    x .+ r*sin.(theta), y .+ r*cos.(theta)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function circles_overlap(p1,r1,p2,r2)\n",
    "    dx = p2[1] - p1[1]\n",
    "    dy = p2[2] - p1[2]\n",
    "    d = sqrt(dx*dx+dy*dy)\n",
    "    rtn = false\n",
    "    r12 = r1+r2\n",
    "    if (d < r12)\n",
    "        # circles intersect\n",
    "        rtn =  true\n",
    "    end\n",
    "    return d, rtn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic Collision\n",
    "\n",
    "<center><img src=\"imgL9/elastic.gif\" style=\"center\" width=50%/></center>\n",
    "(image reference: wikipedia)\n",
    "\n",
    "In one dimensional Newtonian motion, we can consider two particles with $m_1$ and $m_2$ masses with $u_1$ and $u_2$ velocities before a collision and with $v_1$ and $v_2$ velocities after a collision.\n",
    "\n",
    "As the total momentum is conserved before and after collision, we can write\n",
    "\n",
    "$$ m_1u_1+m_2u_2 = m_1v_1+m_2v_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgL9/elastic.gif\" style=\"center\" width=40%/></center>\n",
    "\n",
    "Similarly, the total kinetic energy of the particles is also conserved and this can be given with\n",
    "\n",
    "$$ {1\\over2}m_1u_1^2+ {1\\over2}m_2u_2^2= {1\\over2}m_1v_1^2+ {1\\over2}m_2v_2^2$$\n",
    "\n",
    "Solving these equations, we can determine $v_1$ and $v_2$ if $u_1$ and $u_2$ is known with\n",
    "\n",
    "$$ v_1 = {{N}\\over{M}}u_1+{{2m_2}\\over{M}}u_2$$ $$v_2 = {{2m_1}\\over{M}}u_1+{{-N}\\over{M}}u_2$$\n",
    "\n",
    "where $M = m_1+m_2$ and $N = m_1-m_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function checkCollision(balls)\n",
    "    any_collision = false\n",
    "    collisions = []\n",
    "    # First check of all collisions\n",
    "    for ballpair in combinations(1:length(balls), 2)\n",
    "        balla, ballb = balls[ballpair[1]], balls[ballpair[2]]\n",
    "        distance, overlap = circles_overlap(balla.pos, balla.r, ballb.pos, ballb.r)\n",
    "        if overlap\n",
    "            any_collision = true\n",
    "            collision = (ballb.pos - balla.pos) / distance\n",
    "            if balla.id == 1\n",
    "                balla.vel[1] = sign(rand() - 0.5) * (4.0 + 4.0 * rand())\n",
    "                balla.vel[2] = 3.0 + rand() * 2.0\n",
    "            end\n",
    "            if ballb.id == 1\n",
    "                ballb.vel[1] = sign(rand() - 0.5) * (4.0 + 4.0 * rand())\n",
    "                ballb.vel[2] = 3.0 + rand() * 2.0\n",
    "            end\n",
    "            aci = dot(balla.vel, collision)\n",
    "            bci = dot(ballb.vel, collision)\n",
    "            # new velocities using the 1-dimensional elastic collision equations\n",
    "            acf = bci * 2.0 * balla.m / (balla.m + ballb.m)\n",
    "            bcf = aci * 2.0 * ballb.m / (balla.m + ballb.m)\n",
    "            # Packing collisions here to update it later \n",
    "            pack = [\n",
    "                ballpair[1], (acf - aci) * collision,\n",
    "                ballpair[2], (bcf - bci) * collision\n",
    "            ]\n",
    "            push!(collisions, pack)\n",
    "        end\n",
    "    end\n",
    "    # Next update all collisions\n",
    "    for coll in collisions\n",
    "        # replace the velocity components\n",
    "        if coll[1] != 1\n",
    "            balls[coll[1]].vel += coll[2]\n",
    "        end\n",
    "        if coll[3] != 1\n",
    "            balls[coll[3]].vel += coll[4]\n",
    "        end\n",
    "    end\n",
    "    return any_collision, balls\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we update the ball positions and velocities according to the collisions and constrains on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "function updateBalls(env)\n",
    "    w = env[3][2]\n",
    "    h = env[3][3]\n",
    "    g = env[3][4]\n",
    "    frame_second = env[3][5]\n",
    "    any_collision, balls = checkCollision(env[1])\n",
    "    touched_ground = false\n",
    "    for (bid, ball) in enumerate(balls)\n",
    "        if ball.id != 1\n",
    "            ball.vel[2] -= g * frame_second\n",
    "            ball.pos += ball.vel * frame_second\n",
    "        else\n",
    "            ball.pos[1] += ball.vel[1] * frame_second\n",
    "        end\n",
    "        # left edge\n",
    "        if (ball.pos[1] <= ball.r)\n",
    "            ball.pos = [ball.r, ball.pos[2]]\n",
    "            ball.vel = [-ball.vel[1], ball.vel[2]]\n",
    "        end\n",
    "        # right edge\n",
    "        if (ball.pos[1] >= (w - ball.r))\n",
    "            ball.pos = [w - ball.r, ball.pos[2]]\n",
    "            ball.vel = [-ball.vel[1], ball.vel[2]]\n",
    "        end\n",
    "        # bottom edge\n",
    "        if (ball.pos[2] <= ball.r)\n",
    "            ball.pos = [ball.pos[1], ball.r]\n",
    "            ball.vel = [ball.vel[1], -ball.vel[2]]\n",
    "            if ball.id != 1\n",
    "                touched_ground = true\n",
    "            end\n",
    "        end\n",
    "        # top edge\n",
    "        if (ball.pos[2] >= (h - ball.r))\n",
    "            ball.pos = [ball.pos[1], h - ball.r]\n",
    "            ball.vel = [ball.vel[1], -ball.vel[2]]\n",
    "        end\n",
    "        if ball.id == 1\n",
    "            #balls[bid].vel[1] = sign(rand() - 0.5) * (2.0 + 4.0 * rand())\n",
    "            #balls[bid].vel[2] = 1.0 + rand() * 2.0\n",
    "            balls[bid].vel[1] = 0.0\n",
    "            balls[bid].vel[2] = 0.0\n",
    "        else\n",
    "            balls[bid].pos = ball.pos\n",
    "            balls[bid].vel = ball.vel * 0.9995\n",
    "        end\n",
    "    end\n",
    "    env[1] = balls\n",
    "    if env[2] == 0 && touched_ground\n",
    "        # Gameover\n",
    "        #env[2] = 1\n",
    "        if env[3][1] > 0\n",
    "            env[3][1] -= 0.5\n",
    "        end\n",
    "    end\n",
    "    if env[2] == 0 && any_collision\n",
    "        # Update score\n",
    "        env[3][1] += 1\n",
    "    end\n",
    "    return env\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment than is set up as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function game_setup(;time=nothing)\n",
    "    # Initial values and environmental settings of the game\n",
    "    w = 4 # width in m\n",
    "    h = 5 # height in m\n",
    "    ballradius = 0.2 # m\n",
    "    ballmass = 0.01 # kg\n",
    "    frame_second = 0.001 # s\n",
    "    if time == nothing\n",
    "        n = 10000 # number of frames\n",
    "    else\n",
    "        n = floor(time/0.001)\n",
    "    end\n",
    "    g = 9.807 # m / s^2 gravity on Earth\n",
    "    snapshot_interval = 50\n",
    "    score = 0\n",
    "    gameover = 0 # the game is over if 1\n",
    "    \n",
    "    # Generating initial positions and velocities of the balls\n",
    "    rnd = rand(4)\n",
    "    ball1x  = rnd[1] * (w-2.0*ballradius) + ballradius\n",
    "    ball1y  = rnd[2] * (h*0.5-ballradius) + h*0.49\n",
    "    #ball1vx = 4.0 * (rnd[3] * 2.0 - 1.0)\n",
    "    ball1vx = sign(rand() - 0.5) * (3.0 + 1.0 * rand())\n",
    "    ball2x  = rnd[4] * (w-2.0*ballradius) + ballradius\n",
    "    if ball1x > w*0.5 && ball2x > w*0.5\n",
    "        ball1x -= w*0.5\n",
    "    elseif ball1x < w*0.5 && ball2x < w*0.5\n",
    "        ball1x += w*0.5\n",
    "    end\n",
    "\n",
    "    # First ball is player\n",
    "    balls = [\n",
    "        # set initial position and velocity vectors\n",
    "        Ball(1, [ball2x, ballradius], [0, 0], :red, ballradius, ballmass),\n",
    "        Ball(2, [ball1x, ball1y], [ball1vx, 0.0], :purple, ballradius, 10*ballmass),\n",
    "        #Ball(3, [ball1x, ball1y], [0.0, 0.0], :blue, ballradius, ballmass),\n",
    "    ]\n",
    "    \n",
    "    settings = [score, w, h, g, frame_second]\n",
    "    sim_time = [n, snapshot_interval]\n",
    "    env = [balls, gameover, settings, sim_time]\n",
    "    return env\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The visualisation of the environment is provided through the convertion of snapshots into a `gif` animation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function game_visualise(snapshots, env)\n",
    "    w = env[3][2]\n",
    "    h = env[3][3]\n",
    "    last_id = 1\n",
    "    @gif for i=1:length(snapshots)\n",
    "        time = snapshots[i][1]\n",
    "        touched_ground = snapshots[i][2]\n",
    "        player_score = snapshots[i][3]\n",
    "        balls = snapshots[i][4]\n",
    "        #if !touched_ground\n",
    "        #    last_id = i\n",
    "        #else\n",
    "        #    balls = snapshots[last_id+1][4]\n",
    "        #end\n",
    "        count = 1\n",
    "        for ball in balls\n",
    "            if count == 1\n",
    "                plot(circle(ball[1][1],ball[1][2],ball[2]), seriestype = [:shape], \n",
    "                     lw = 0.5, c = ball[3], linecolor = :black, legend = false, \n",
    "                     fillalpha = 0.2, aspect_ratio = 1, xlim=(0, w), ylim=(0, h))\n",
    "            else\n",
    "                plot!(circle(ball[1][1],ball[1][2],ball[2]), seriestype = [:shape], \n",
    "                      lw = 0.5, c = ball[3], linecolor = :black, legend = false, \n",
    "                    fillalpha = 0.2, aspect_ratio = 1, xlim=(0, w), ylim=(0, h))\n",
    "            end\n",
    "            count += 1\n",
    "        end\n",
    "        str = @sprintf \"Time: %3.1f\" time\n",
    "        annotate!([(w-(w*0.15), h-(0.05*h), Plots.text(str, 10, :black, :left))])\n",
    "        score_str = @sprintf \"Score: %d\" player_score\n",
    "        annotate!([(w-(w*0.15), h-(0.10*h), Plots.text(score_str, 10, :black, :left))])\n",
    "        #if touched_ground == 1\n",
    "        #    if i%3 == 0\n",
    "        #        annotate!([(w*0.26, h*0.5, Plots.text(\"GAME OVER\", 16, :red, :left))])\n",
    "        #    elseif i%3==1\n",
    "        #        annotate!([(w*0.26, h*0.5, Plots.text(\"GAME OVER\", 16, :blue, :left))])\n",
    "        #    else\n",
    "        #        annotate!([(w*0.26, h*0.5, Plots.text(\"GAME OVER\", 16, :orange, :left))])\n",
    "        #    end\n",
    "        #end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simulation (game) environment is controlled through the function defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function game_run(env; brain=nothing)\n",
    "    snapshots = []\n",
    "    time = 0.0\n",
    "    for i=1:env[4][1]\n",
    "        if brain != nothing\n",
    "            state = get_state(env)\n",
    "            a = action(state) # ANN is here!\n",
    "        else\n",
    "            a = nothing\n",
    "        end\n",
    "        env = game_update(env; action=a)\n",
    "        if i==1 || i%env[4][2]==0 || i==env[4][1]\n",
    "            push!(snapshots,[time, env[2], env[3][1],\n",
    "                    [[b.pos,b.r,b.color] for b in env[1]]])\n",
    "        end\n",
    "        time += env[3][5]\n",
    "    end\n",
    "    return env, snapshots\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function get_state(env)\n",
    "    state = [\n",
    "        env[1][1].pos[1], # x position of player \n",
    "        #env[1][1].vel[1], # x velocity of player\n",
    "        #env[2],           # Score of player\n",
    "        #env[3][1],        # Time in game\n",
    "        env[1][2].pos[1], # x position of the other ball\n",
    "        env[1][2].pos[2]  # y position of the other ball\n",
    "        #env[1][2].vel[1], # x velocity of the other ball\n",
    "        #env[1][2].vel[2]  # y velocity of the other ball\n",
    "    ]\n",
    "    return state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function game_update(env; action=nothing)\n",
    "    env = updateBalls(env)\n",
    "    \n",
    "    # Auto follow bouncing ball\n",
    "    if env[1][1].pos[1] > env[1][2].pos[1]\n",
    "        env[1][1].vel[1] -= 4.0\n",
    "    end\n",
    "    if env[1][1].pos[1] < env[1][2].pos[1]\n",
    "        env[1][1].vel[1] += 4.0\n",
    "    end\n",
    "    \n",
    "    return env\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function game()\n",
    "    env = game_setup()\n",
    "    env, snapshots = game_run(env)\n",
    "    game_visualise(snapshots, env)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function game(;brain=nothing, time=nothing)\n",
    "    env = game_setup(;time=time)\n",
    "    env, snapshots = game_run(env,brain=brain)\n",
    "    game_visualise(snapshots, env)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function game_update(env; action=nothing)\n",
    "    env = updateBalls(env)\n",
    "    if action != nothing\n",
    "        if action[1] < 0.0\n",
    "            # move left\n",
    "            env[1][1].vel[1] -= 4.0\n",
    "        end\n",
    "        if action[1] > 0.0\n",
    "            # move right\n",
    "            env[1][1].vel[1] += 4.0\n",
    "        end\n",
    "    end\n",
    "    return env\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We will use `Flux` and `Zygote` to train our ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using Flux.Tracker: track, @grad, data, gradient\n",
    "#using Flux.Optimise: Optimiser, _update_params!, update!\n",
    "using Statistics: mean\n",
    "import Base.sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The states that will be used in the agent training is defined with the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_state(env)\n",
    "    state = [\n",
    "        (4.0 * env[1][1].pos[1]/env[3][2] - 2.0), # normalised x position of player \n",
    "        env[1][1].vel[1], # x velocity of player\n",
    "        #env[1][1].vel[2], # y velocity of player\n",
    "        (4.0 * env[1][2].pos[1]/env[3][2] - 2.0), # normalised x position of the other ball\n",
    "        env[1][2].pos[2], # normalised y position of the other ball\n",
    "        env[1][2].vel[1], # x velocity of the other ball\n",
    "        env[1][2].vel[2], # y velocity of the other ball\n",
    "        #env[2]           # Gameover\n",
    "        #env[3][1]+1.0     # current score\n",
    "    ]\n",
    "    return state\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First, we setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env = game_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Parameters -------------------------------------\n",
    "STATE_SIZE = length(get_state(env))\n",
    "SEQ_LEN = 10 # attempts to play game in each learning step\n",
    "GEN_LEN = 100\n",
    "SIM_TIME = 4 # seconds\n",
    "MAX_TRAIN_REWARD = SIM_TIME * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Second, we build our ANN model, define optimization method and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------ Model Architecture ----------------------------\n",
    "\n",
    "sign(x::TrackedArray) = track(sign, x)\n",
    "@grad sign(x) = Base.sign.(data(x)), xÌ -> (xÌ,)\n",
    "\n",
    "model = Chain(Dense(STATE_SIZE, 128, tanh),\n",
    "              Dense(128, 128, tanh),\n",
    "              Dense(128, 1, tanh), sign)\n",
    "\n",
    "# Optimiser params\n",
    "Î· = 0.01\n",
    "\n",
    "opt = ADAM(Î·)\n",
    "\n",
    "action(state) = model(state)\n",
    "\n",
    "function loss(rewards)\n",
    "  ep_len = size(rewards, 1)\n",
    "  max_rewards = ones(ep_len) * MAX_TRAIN_REWARD\n",
    "  return Flux.mse(rewards, max_rewards)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While Flux provides training function, for complex models such as in here, one can use `Zygote` directly with the following helper functions. These functions `updates` the model parameters, the `weights` and `biases`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------- Helper Functions -------------------------------\n",
    "\n",
    "# Recursive zygote update method\n",
    "function zyg_update!(opt, model, updates)\n",
    "    if nfields(model) == 0\n",
    "        return model\n",
    "    end\n",
    "    \n",
    "    # If it does have fields, recurse into them:\n",
    "    for field_idx in 1:nfields(model)\n",
    "        zyg_update!(opt, getfield(model, field_idx), getfield(updates, field_idx))\n",
    "    end\n",
    "    \n",
    "    return model\n",
    "end\n",
    "\n",
    "zyg_update!(opt, model, updates::Nothing) = model\n",
    "\n",
    "function zyg_update!(opt, model::AbstractArray, updates::AbstractArray)\n",
    "    # Sub off to Flux's ADAM optimizer\n",
    "    Flux.Optimise.apply!(opt, model, updates)\n",
    "    return model .-= updates\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### As in Markov Desicion Process, we need to define the `reward` of transition from `s` to `s'` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "function train_reward(env, time)\n",
    "    score  = env[3][1]  # Score of player\n",
    "    if score < 0\n",
    "        score = 0\n",
    "    end\n",
    "    return score\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A common approach to train an ANN model for agent learning is to use `replay` training.\n",
    "\n",
    "In replay training, the model is trained for a given period of the past states and their rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function replay(rewards)\n",
    "    grads = Zygote.gradient(model) do model\n",
    "        return loss(rewards)\n",
    "    end\n",
    "    # Apply recursive update to our model:\n",
    "    zyg_update!(opt, model, grads)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The overall simulation and the on-the-fly training of agent is provided with the following function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "function generation!(train=true;ngen=nothing)\n",
    "    total_reward = 0f0\n",
    "    attempt = 0\n",
    "    if ngen == nothing\n",
    "        ngen = GEN_LEN\n",
    "    end\n",
    "    while attempt <= ngen\n",
    "        env = game_setup()\n",
    "        env[4][1] = floor(SIM_TIME/env[3][5]) + 1\n",
    "        rewards = []\n",
    "        time_step = 0\n",
    "        current_time = 0.0\n",
    "        while current_time <= SIM_TIME\n",
    "            state = get_state(env)\n",
    "            \n",
    "            a = action(state) # ANN is here!\n",
    "            env = game_update(env, action=a)\n",
    "            \n",
    "            #r = train_reward(env, current_time)\n",
    "\n",
    "            current_time = time_step * env[3][5]\n",
    "            #if env[2] == 1 # Gameover\n",
    "            #    break\n",
    "            #end\n",
    "            time_step += 1\n",
    "        end\n",
    "        r = train_reward(env, current_time)\n",
    "        total_reward += r\n",
    "        train && push!(rewards, r)\n",
    "        if train && (attempt%SEQ_LEN == 0 || attempt == ngen )\n",
    "            #println(\"Training at $attempt | Current Score: $total_reward | \")\n",
    "            rewards = vcat(rewards...)\n",
    "            replay(rewards) # Train all the results of previous games so far\n",
    "            rewards = []\n",
    "        end\n",
    "        attempt += 1\n",
    "    end\n",
    "    return total_reward\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To test the agent on new random states, we define a `test` function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Testing -------------------------------------\n",
    "\n",
    "function test()\n",
    "    score_mean = 0f0\n",
    "    total_reward = generation!(false, ngen=100)\n",
    "    score_mean += total_reward / 100\n",
    "    return score_mean\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we train our agent within the developed environment using defined `rewards` and `states`. \n",
    "The actions of agent is taken by the ANN model, hence the agent is considered an AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Training --------------------------------------\n",
    "@save \"BALLBOUNCE-model-checkpoint5.bson\" model\n",
    "e = 1\n",
    "while true\n",
    "    total_reward = generation!()\n",
    "    println(\"Generation: $e | Score: $total_reward | \")\n",
    "    @save \"BALLBOUNCE-model-checkpoint5.bson\" model\n",
    "    score_mean = test()\n",
    "    score_mean_str = @sprintf \"%6.2f\" score_mean\n",
    "    println(\"Mean score over $GEN_LEN test games: \" * score_mean_str)\n",
    "    \n",
    "    if score_mean > 4.0\n",
    "        println(\"Found best gamer!\")\n",
    "        break\n",
    "    end\n",
    "    e += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "game(brain=model, time=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Study Topics:\n",
    "1. Use Neural Networks to build an `AI-pilot` for **Mars Lander** in assigment 4 in Lecture 4 ([L4-N-Body-Problem](L4-N-Body-Problem.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
