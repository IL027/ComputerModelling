{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IL027 Interdisciplinary Computer Modelling\n",
    "\n",
    "## Lecture 9 (Part 1) - Artificial Neural Networks\n",
    "\n",
    "### Berk Onat \n",
    "#### Warwick Predictive Modelling Centre, School of Engineering\n",
    "##### Email: B.Onat@warwick.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <hr><center>Artificial Neural Networks (ANN)</center><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### This lecture covers the following topics:\n",
    "- **Linear Regression**\n",
    "- **Artificial Neural Networks**\n",
    "  - Exercise 1: ANNs for regression using Boston Housing Data\n",
    "  - Exercise 2: Classification of Hand-written Digits using MNIST data\n",
    "- **Reinforcement Learning (in Part 2)**\n",
    "  - Build an AI based on ANN to play with a ball.\n",
    "  - Exercise 3: Train AI to play with a ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Artificial Intelligence?\n",
    "\n",
    "Artificial Intelligence is the concept of building machines that demonstrate intelligence as colser to human intelligence where they can mimic **cognitive functions of humans** such as **learning** and **problem solving**. AI as any intelligent device, can **learn taking actions in an environment** to maximize its chance **to achieve a goal**. (Adapted from wikipedia)\n",
    "\n",
    "### What is Machine Learning?\n",
    "\"<em><b>Machine learning</b></em> is the practice of using algorithms to <b>parse data, learn from it</b>, and then make <b>predictions about the output of interest</b>. So rather than hand-coding software routines with a specific set of instructions to accomplish a particular task, the machine is <b>“trained”</b> using large amounts of data and algorithms that give it the ability to learn how to perform the task.\" (Reference: nvidia.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the difference between <span style=\"color:red\">Artificial Intelligence</span>, <span style=\"color:blue\">Machine Learning</span>, <span style=\"color:green\">Deep Learning</span>, and <span style=\"color:orange\">Data Science</span>?\n",
    "<img src=\"imgL9/ai-machinelearning.png\" style=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Function Fitting\n",
    "\n",
    "The simplest model for regression is the one in which the target value is expected to be a `linear combination of the input variables`\n",
    "\n",
    "$$y(w,x) = w_0 + w_1x_1 + w_2x_2 + ... +w_Nx_N $$\n",
    "\n",
    "Where $w = (w_0,w_1,...,w_N)$ as the `weights` of the model and $X = (x_0,x_1,x_2,...,x_N)^T$ input values where $x_0$ is 1. Here the input values can be accounted a vector and $w$ set has N+1 values. We can simplify the notation using a sum as\n",
    "\n",
    "$$y(w,x) = \\sum_{j=0}^{N} w_jx_j = w^TX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In linear closed-solutions, we can find the `weights` , $w$ by solving\n",
    "\n",
    "$$w^TX-y = 0$$\n",
    "\n",
    "Multiply $X$ from left and $X^T$ from right, we get\n",
    "\n",
    "$$ X(w^TX)X^T - XyX^T = 0 $$\n",
    "\n",
    "and take the transpose of both sides and multiply with $(X^T)^{-1}$ from right\n",
    "\n",
    "$$ wX^TX = Xy^T =X^Ty$$\n",
    "\n",
    "and rearrange,\n",
    "\n",
    "$$ w = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's generate a sample data to test `Linear Regression`. Here we take random $X$ values and use  \n",
    "\n",
    "$$ true\\_fun(X) = 1.5X + 2$$ \n",
    "\n",
    "as the function to produce true $y$ values with some random additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_samples=100 # number of samples in the dataset\n",
    "\n",
    "true_fun(X) = 1.5 * X + 2.0\n",
    "\n",
    "x = sort(rand(n_samples))\n",
    "y = true_fun.(x) .+ randn(n_samples) .* 0.1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "scatter(x,y, label=\"Given data\")\n",
    "plot!(x,true_fun.(x), label=\"True function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "linreg(x, y) = hcat(fill!(similar(x), 1), x) \\ y\n",
    "# y = ax + b\n",
    "fit_func(a,b,x) = a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(b, a) = linreg(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(x,y, label=\"Given data\")\n",
    "plot!(x,true_fun.(x), color=\"green\", lw=2, label=\"True function\")\n",
    "plot!(x,fit_func.(a,b,x), color=\"red\", lw=2, label=\"Fitted function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression with Basis Functions\n",
    "Note that the model is as simple as a linear function of only the input values here. To generelise the model we can also consider that $y$ can be represented with `linear combinations of linear or non-linear functions`. \n",
    "\n",
    "$$y(w,x) = w_0 + \\sum_{j=1}^{N-1} w_j\\phi_j(x)$$\n",
    "\n",
    "where $\\phi(x)$ is called as `basis function`. We can rewrite this in the following form:\n",
    "\n",
    "$$y(w,x) = \\sum_{j=0}^{N-1} w_j\\phi_j(x)$$\n",
    "\n",
    "and also represent it with an inner product of $w$ and $\\phi(x)$ vectors as\n",
    "\n",
    "$$y(w,x) = w^T\\phi(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a linear model\n",
    "\n",
    "We would like to find the optimum `weights` for our linear model that produce `y` values. \n",
    "If $y$ is linearly dependent to $x$ the solution will be exactly find and the equation above will hold the equality. However, if we do not know whether $y(x)$ is a linear function, we can define the problem to be\n",
    "\n",
    "$$\\hat y = y(w,x) + E(w)$$\n",
    "\n",
    "where $\\hat y$ is the corresponding target values for each data point $x$ and $E$ is the `error function` for the model with `weights` and defined as sum-of-squares with\n",
    "\n",
    "$$E(w) = {1 \\over 2}\\sum_{n=1}^{N}[y(x_n,w) - \\hat y_n]^2$$\n",
    "\n",
    "Minimizing $E(w)$, we can find the optimum `weights` and fit our model. Minimizing a sum-of-squares error function can be considered as the `least squares` approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression with Least Squares\n",
    "\n",
    "For non-linear proglems in `least squares` approach, we can minimize the same `error function` \n",
    "\n",
    "$$ \\min_{w}(E(w))$$\n",
    "\n",
    "or we can minimize mean-squared-error (MSE) definiton as\n",
    "\n",
    "$$ \\min_{w}(2E(w)/N)$$\n",
    "\n",
    "\n",
    "starting an initial guess for $w$. Generally, we want weights to be small therefore we will select a random distribution around zero for initial guess of `weights`.\n",
    "\n",
    "$$ w^{k+1}_j = w^k_j + \\Delta w^k_j$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Auto Gradient of Functions with <span style=\"color:green\">Zygote</span> \n",
    "Zygote is a package for source-to-source automatic differentiation (AD) in Julia\n",
    "\n",
    "<img src=\"imgL9/zygote.png\" style=\"center\" width=40%/>\n",
    " \n",
    "For documentation of Zygote, please check <a>https://fluxml.ai/Zygote.jl</a>\n",
    "\n",
    "To access the code distribution see <a>https://github.com/FluxML/Zygote.jl</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try a simple example function to find the derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = cos.(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(x -> f(x), pi/2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = gradient(x -> f(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(pi/2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-sin(pi/2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can define our `linear regression` model using `gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our linear regression model\n",
    "predict_y(w, x) = x * w'\n",
    "\n",
    "# error function\n",
    "error_function(w, x, y) = sum(abs2.(y - predict_y(w, x))) / size(x, 1)\n",
    "\n",
    "function linear_regression(w, x, y; lr=1.0E-2, N=1000)\n",
    "    for i = 1:N\n",
    "        dw = gradient(w -> error_function(w,x,y), w)[1]\n",
    "        w = w - (lr * dw)\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Adding a column of ones for b (constant) in y=a*x+b \n",
    "x_input = hcat(x, ones(size(x,1))) \n",
    "\n",
    "# Initialise random weights within (-1,1] range \n",
    "w = rand(1, size(x, 2)+1) .* 2.0 .- 1.0\n",
    "\n",
    "#Fit linear regression model and print final error\n",
    "w = linear_regression(w, x_input, y, lr=1.0E-1, N=100000)     \n",
    "println(error_function(w, x_input, y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(x,y, label=\"Given data\")\n",
    "plot!(x,true_fun.(x), color=\"green\", lw=2, label=\"True function\")\n",
    "plot!(x,predict_y(w, x_input), color=\"red\", lw=2, label=\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<span style=\"color:red\">Flux</span> is a Machine Learning (ML) library in Julia that provides a fast differentiable programming framework.\n",
    "\n",
    "<img src=\"imgL9/flux.png\" style=\"center\" width=40%/>\n",
    "\n",
    "See <a>https://github.com/FluxML/Flux.jl</a> to access the code distribution.\n",
    "\n",
    "Please check <a>https://fluxml.ai/Flux.jl</a> for Flux documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Tracker: track, @grad, data, gradient\n",
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated\n",
    "import Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Artificial Neural Networks (ANN) or in short Neural Networks (NN) are inspired from actual <b>neurons</b> in nerve system. While a <b>neuron</b> can be activated with an input pulse signal, it responds non-linearly to this signal and produces an output signal or in other words value. \n",
    "\n",
    "In NNs, this idea is simplified to additive value description of inputs with a non-linear responce function ,which we call <b>activation function</b>. The building block of NNs is called <b>perceptrons</b> as show below.\n",
    "\n",
    "<center><img src=\"imgL9/perceptron.png\" style=\"center\" width=30%/></center>\n",
    "\n",
    "Mathematically, a perceptron is defined with \n",
    "$$ y_{w,b}(x) = f(\\sum_i^N x_iw_i + b_i)$$\n",
    "In the example above, number of values in input vector $x$ is $N=3$ and $w$ values are the <b>weight</b> parameters with $b$ as the <b>bias weight</b> parameter. An optimization can be applied on <b>weights</b> and <b>bias weights</b> of the network according to the input vector and output target value(s). For simplicity, we can also call each perceptron connection (orange circle in Figure) a <b>node</b> as in graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "In the above equation, $f(x)$ function is the <b>activation function</b> and in general it is taken as the $tanh(x)$ function or a similar non-linear/switching function such as sigmoid function (Logistic) or Rectified Linear Unit (ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -4.0:0.01:4.0\n",
    "plot(x,relu.(x), color=\"blue\", linewidth=3, label=\"ReLU\")\n",
    "plot!(x,σ.(x), color=\"orange\", linewidth=3, label=\"Sigmoid\")\n",
    "plot!(x,tanh.(x), color=\"red\", linewidth=3, label=\"Tanh\")\n",
    "ylims!((-2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Layer Perceptrons (MLP) aka Neural Networks\n",
    "\n",
    "Using the building block perceptron, we can connect many nodes in multiple layers. When we connect perceptrons with many layers, this will be called Multi-Layer Perceptrons (MLPs) and in general Neural Networks are built on them. Examine the example below in figure.\n",
    "\n",
    "<center><img src=\"imgL9/nn.png\" style=\"center\" width=30%/></center>\n",
    "\n",
    "Here, we build a neural network with <b>4</b> layers. The first layer at the left hand side is the <b>input layer</b> (blue spheres, nodes) and is the $x$ input vector of the model. At far right hand side, we have <b>output layer</b> (red spheres, nodes) where the model produces $y$ output values ,which are also the predictions of the model. In between these layers, there are two <b>hidden layers</b> that are full-connected to each other.\n",
    "\n",
    "In equation form, the above network can be written as\n",
    "\n",
    "$$ y_{w,b}(x) = f(\\sum_i^N w_if(\\sum_j^M x_jw_j + b_j) + b_i)$$\n",
    "\n",
    "or in recursion equation form, this will be\n",
    "\n",
    "$$ y_{w,b}^{k+1}(x) = f(\\sum_i^{N_k} w_iy_i^{k} + b_i^{k})$$\n",
    "\n",
    "where $k=0,...,2$ and $y^0=x$ and $y^2=y$.\n",
    "\n",
    "While the input layer is also full-connected to each node of the hidden layer, the output layer is only connected to the outputs of each last hidden layer. This is because of the red arrow shows the connections hence the <b>weights</b> of each input, while the last output is just a sum over the output values of each hidden node. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent optimization\n",
    "In order to minimize/optimize $E(w)$ <b>the loss (objective) function</b>, we consider the gradient descent (GD) with:\n",
    "\n",
    "$$ w_{k+1}=w_{k}-\\epsilon \\nabla E(w_k)$$\n",
    "\n",
    "In each iteration of the gradient descent step, the optimization is carried-out on different sets of data hence the `weights` of the model is updated with respect to the selected random data set. Hence randomly selecting the $x_i$ values in the Gradient Descent is called Stochastic Gradient Descent (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation Algorithm.\n",
    "To optimize the `weights` of ANN, we use `Backpropagation Algorithm` to back-propagate error.\n",
    "Than, we can find the derivative of `loss function` with respect to the weigths.\n",
    "\n",
    "Note, the recurrence function of ANN is defined with\n",
    "\n",
    "$$ y_{w,b}^{k+1}(x) = f(\\sum_i^{N_k} w_iy_i^{k} + b_i^{k})$$\n",
    "\n",
    "Than, we can write derivative of $E$ with respect to `weights`\n",
    "$${{\\partial E}\\over{\\partial w^k}} = {{\\partial E}\\over{\\partial y^k}}{{\\partial y^k}\\over{\\partial w^k}}$$\n",
    "\n",
    "and `baises`\n",
    "\n",
    "$${{\\partial E}\\over{\\partial b^k}} = {{\\partial E}\\over{\\partial y^k}}{{\\partial y^k}\\over{\\partial b^k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Networks for Regression using Boston Housing Data (boston.csv)\n",
    "This is a copy of UCI ML housing dataset. [https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df = DataFrames.readtable(\"boston.csv\")\n",
    "first(boston_df,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "boston = convert(Matrix,boston_df);\n",
    "boston_input = boston[:,1:13];\n",
    "boston_target = boston[:,14:14];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(boston_input[:,3],boston_target, \n",
    "    xlabel=\"Distance to Industry Zone\",\n",
    "    ylabel=\"Property Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Flux, a model can be a dense matrix with `Dense(m,n)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Dense(13, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or a chain of functions defined in `Chain(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Chain(\n",
    "    Dense(13, 1),\n",
    "    x -> x.^2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(13, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Before using the given data, we can normalise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the data\n",
    "x_input = (boston_input .- mean(boston_input, dims = 2)) ./ std(boston_input, dims = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Flux, an ANN model definition is simple as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Model is define here\n",
    "model = Chain(\n",
    "    Dense(13, 1)\n",
    "    )\n",
    "\n",
    "# Define loss (error) function\n",
    "loss(x, y) = mean((model(x) .- y).^2)\n",
    "\n",
    "# Setting multiple instance of data for optimization\n",
    "dataset = repeated((x_input', boston_target'), 1000)\n",
    "\n",
    "# This function is a callback function to print loss values\n",
    "evalcb = () -> @show(loss(x_input', boston_target'))\n",
    "\n",
    "# We set ADAM optimization method\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now, we can train our model using the defined functions and parameters.\n",
    "\n",
    "Here we train the model 100 steps or `epochs` as it is defined in machine learning community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for i=1:100\n",
    "    Flux.train!(loss, params(model), dataset, opt, cb = throttle(evalcb, 1000))\n",
    "end\n",
    "@show loss(x_input', boston_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can check ANN predictions by comparing predicted data with target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predicted = Tracker.data(model(x_input'))';\n",
    "scatter(boston_target, predicted)\n",
    "plot!([minimum(boston_target), maximum(boston_target)], \n",
    "    [minimum(boston_target), maximum(boston_target)], \n",
    "    color=\"red\", lw=4, xlabel=\"Measured\", ylabel=\"Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Network as a Classification Model\n",
    "\n",
    "We will use MNIST Hand-Written Digits dataset to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using Colors\n",
    "using Images: channelview, display, colorview\n",
    "#Gray = Colors.Gray{Colors.N0f8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We will download MNIST Hand-written Digits dataset below.\n",
    "If the download process asks you whether you would like to continue with the download, type `y` or `yes` to text window and `enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = MNIST.traindata(dir=\"./\");\n",
    "test_x,  test_y  = MNIST.testdata(dir=\"./\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To visualise the dataset, we will do some convertions and reshaping of arrays below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the image to Float32 Arrays\n",
    "get_images(X) = Float32.(channelview(X))\n",
    "getarray(X) = Float32.(permutedims(channelview(X), (1, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To visualise the dataset, we will do some convertions and reshaping of arrays below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "size(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "imgs = get_images(train_x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = getarray(train_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now, we are ready to visualise the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[Gray.(permutedims(imgs[:,:,i], [2,1])) for i in collect(1:5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also check the corresponding `target` values, which are the integer values of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[collect(1:5)]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Log Likelyhood\n",
    "\n",
    "If we assume that $\\hat y$ target values has a Gaussian distribution with the $x$-dependence, than the maximum log likelihood function can be written with\n",
    "\n",
    "$$ln P(t|w,\\beta) = {N\\over2} ln(\\beta) - {N\\over2} ln(2\\pi) -\\beta E(w)$$\n",
    "\n",
    "Recall that the `error function` $E(w)$ is defined with\n",
    "\n",
    "$$E(w) = {1 \\over 2}\\sum_{n=1}^{N}[y(x_n,w) - \\hat y_n]^2$$\n",
    "\n",
    "If we maximize the logarithm of likelihood, it is equivalent to minimizinf sum-of-squares error function.\n",
    "Moreover, minimizing negative logarithm of likelihood is equivalent to maximizin log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now, we will build our Neural Network with defining loss function (nll, negative log likelyhood) and optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To train all the given training data, we need to stack the images into one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack images into one large batch\n",
    "X = reshape(xx,(:,size(xx)[3]));\n",
    "size(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In this model, we will use `classification` rather than `regression`. To do so, we need to identofy the target values positions in the output arrays.\n",
    "\n",
    "The ANN model can get either a `vector` or a `matrix`. Here, we will supply an input with images, matrices.\n",
    "The output of ANN will hold not the digit numbers but the positions within a vector with size 10. \n",
    "\n",
    "Each output vector of ANN will include values between 0 and 1. The position of the highest value in this vector is the digit encoding. \n",
    "\n",
    "To succesfully encode all the target values and the output values of ANN model, we will use so called `onehot` and `onecold` encodings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In `onehot` encoding, only the values that are given in the function will be shown with `1` and the rest of the values will be coded with `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot-encode the labels\n",
    "Y = onehotbatch(train_y, 0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To reverse this encoding, in other words decoding, we use `onecold` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecold(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now, we are ready to build our ANN model and setup training functions and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(28^2, 4, relu),\n",
    "    Dense(4, 10),\n",
    "    softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## negative log likelihood function\n",
    "loss(x, y) = crossentropy(model(x), y)\n",
    "\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "\n",
    "dataset = repeated((X, Y), 200)\n",
    "evalcb = () -> @show(loss(X, Y))\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Accuracy is defined with the mean number of equal predictions to the actual values. In short, \n",
    "\n",
    "$$ accuracy(y,\\hat y) = \\left(\\sum_i^N \\delta(\\hat y_i - y_i)\\right) / N$$\n",
    "\n",
    "here, `N` is the number of samples. We can calculate the accuracy with `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's train our NN with one hidden layer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, params(model), dataset, opt, cb = throttle(evalcb, 10))\n",
    "@show accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test our model's accuracy, we can use the test dataset from MNIST download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test set accuracy\n",
    "test_imgs = get_images(test_x);\n",
    "txx = getarray(test_x);\n",
    "tX = reshape(txx,(:,size(txx)[3]));\n",
    "tY = onehotbatch(test_y, 0:9);\n",
    "\n",
    "@show accuracy(tX, tY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "size(test_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if we can actually see how ANN can predict what is in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Here, we select the images from 1 (first image) to 5 for our test. \n",
    "If you would like to test other images either change the range in `collect` or assign a list to `selected`. For example, `selected = [10,12,14]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = collect(1:5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "[Gray.(permutedims(test_imgs[:,:,i], [2,1])) for i in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tX[:,selected];\n",
    "y_pred = model(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecold(y_pred).-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "onecold(tY[:,selected]).-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2: Change the number of nodes at hidden layers. \n",
    "### Fill `?` values with appropriate integers below (Hint: select values smaller than 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(28^2, ?, relu),\n",
    "    Dense(?, 10),\n",
    "    softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try fitting this new ANN model by writing your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we trained our new ANN, let's try it on predictions of Digits !\n",
    "\n",
    "### Do outputs match with the target digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do you think the fitting is much better when we select more hidden layers or nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Networks and Deep Learning\n",
    "\n",
    "Addition of multiple layers extend the flexibility of the neural networks to fit complex problems. Multiple hidden layers extend the capability of non-linear representation of the complex problem. Using this idea and pre-processing layers such as convolution of input values over non-dependent spacial dimensions and gradient back-propagation, deep neural networks could be constructed. These deep neural networks is shown that they can be used in image recognition very efficiently and very accurately. <b>Deep Learning</b> as a sub field of ML focus on these deep networks and their possible applications. \n",
    "\n",
    "For further reading about Deep Learning, here are some references: [Deep Learning](http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/), [Deep Convolutional Networks](http://deeplearning.net/tutorial/lenet.html#lenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Study Topics:\n",
    "1. Use data from `Student Performance in Exams` in [Kaggle](https://www.kaggle.com/spscientist/students-performance-in-exams) and cluster the data using `Classification Models` according to different features. You may answer whether the following has any effect on the exam scores: the student had a lunch before the exam, took exam preperation courses or their parental level of education.\n",
    "2. How about predicting properties of Pokemon monsters? [The Complete Pokemon Dataset](https://www.kaggle.com/rounakbanik/pokemon) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For further reading on this topic:\n",
    "\n",
    "1. **Autoencoder neural networks** in unsupervised learning. [Tutorial on Autoencoders at Stanford University](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)\n",
    "2. **Self-Orginized Maps (SOM)** for clustering data in unsupervised learning. Also check the Julia code here that I arranged for SOM [SOM Julia code](https://github.com/berkonat/julia-ml-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
