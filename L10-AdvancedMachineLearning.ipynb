{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IL027 Interdisciplinary Computer Modelling\n",
    "\n",
    "## Lecture 10 - Advanced Machine Learning\n",
    "\n",
    "### Michael Castelle - Centre for Interdisciplinary Methodologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant fetch_openml\n"
     ]
    }
   ],
   "source": [
    "using Knet, Plots, Images\n",
    "using ScikitLearn\n",
    "@sk_import datasets: fetch_openml\n",
    "gr(fmt=:png)\n",
    "\n",
    "srand(1234); # seed random number generator for reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Stuff\n",
    "\n",
    "In this notebook we introduce the following Julia/Knet packages and functions:\n",
    "\n",
    "* Knet's function [conv4](https://knet.readthedocs.io/en/latest/cnn.html#convolution): Execute convolutions or cross-correlations using filters specified with `w` over tensor `x`.\n",
    "* Knet's function [pool](https://knet.readthedocs.io/en/latest/cnn.html#pooling): Compute pooling of input values (i.e., the maximum or average of several adjacent values) to produce an output with smaller height and/or width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks (CNNs)\n",
    "\n",
    "In the [previous lecture](L9-MachineLearning.ipynb), we connected the nodes of our neural networks in what seems like the simplest possible way. Every node in each layer was connected to every node in the subsequent layers. \n",
    "\n",
    "![](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/img/multilayer-perceptron.png?raw=true)\n",
    "\n",
    "This can require a lot of parameters! If our input were a 256x256 color image (still quite small for a photograph), and our network had 1,000 nodes in the first hidden layer, then our first weight matrix would require (256x256x3)x1000 parameters. That's nearly 200 million. Moreover the hidden layer would ignore all the spatial structure in the input image even though we know the local structure represents a powerful source of prior knowledge. \n",
    "\n",
    "Convolutional neural networks incorporate convolutional layers. These layers associate each of their nodes with a small window, called a *receptive field*, in the previous layer, instead of connecting to the full layer. This allows us to first learn local features via transformations that are applied in the same way for the top right corner as for the bottom left. Then we collect all this local information to predict global qualities of the image (like whether or not it depicts a dog). \n",
    "\n",
    "![](http://cs231n.github.io/assets/cnn/depthcol.jpeg)\n",
    "(Image credit: Stanford cs231n http://cs231n.github.io/assets/cnn/depthcol.jpeg)\n",
    "\n",
    "In short, there are two new concepts you need to grok here. First, we'll be introducting *convolutional* layers. Second, we'll be interleaving them with *pooling* layers. \n",
    "\n",
    "*First, we reuse (and slightly adapt) some functions from Lecture 9 for loading the digit data, splitting into training and test data, and defining \"minibatches\" of data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y) = fetch_openml(\"mnist_784\", version=1, return_X_y=true)\n",
    "digits_data =reshape(X,(70000,1,28,28))\n",
    "digits_y = [convert(String,y[n]) for n in 1:length(y)]\n",
    "digits_targets = map(x->parse(Int64,x),digits_y)\n",
    "\n",
    "function mnist_split_train_test(x,y,test)\n",
    "    if test == 0\n",
    "        xtrn = xtst = x\n",
    "        ytrn = ytst = y\n",
    "    else\n",
    "        r = randperm(size(x,1))          # trn/tst split\n",
    "        n = round(Int, (1-test) * size(x,1))\n",
    "        xtrn=permutedims(x[r[1:n],:,:,:],(4,3,2,1))\n",
    "        ytrn=y[r[1:n]]\n",
    "        xtst=permutedims(x[r[n+1:end],:,:,:],(4,3,2,1))\n",
    "        ytst=y[r[n+1:end]]\n",
    "    end\n",
    "    return (xtrn, ytrn, xtst, ytst)\n",
    "end\n",
    "\n",
    "function mnist_minibatch(x, y, batchsize; atype=Array{Float32}, \n",
    "                          xrows=28*28, yrows=10, xscale=255)\n",
    "    # NOTE: this is changed from Lecture 9, since the \n",
    "    #   CNN requires 4D input arrays,\n",
    "    #   rather than the 1D arrays used for a standard NN\n",
    "    xbatch(a)=convert(atype, reshape(a./xscale, 28, 28, 1, batch_size))\n",
    "    ybatch(a)=(a[a.==0]=10; \n",
    "               convert(atype, \n",
    "                       sparse(convert(Vector{Int},a),\n",
    "                              1:length(a),one(eltype(a)),yrows,length(a))))\n",
    "    xcols = div(length(x),xrows)\n",
    "    xcols == length(y) || throw(DimensionMismatch())\n",
    "    data = Any[]\n",
    "    for i=1:batchsize:xcols-batchsize+1\n",
    "        j=i+batchsize-1\n",
    "        push!(data, (xbatch(x[1+(i-1)*xrows:j*xrows]), ybatch(y[i:j])))\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "# extract and visualise image j from batch i\n",
    "function mnist_view(d, i, j)\n",
    "    println(d[i][2][:, j], \"  -->  \", findfirst(d[i][2][:, j]))\n",
    "    return Images.colorview(Images.Gray, d[i][1][:,:,1,j])\n",
    "end\n",
    "\n",
    "xtrn, ytrn, xtst, ytst = mnist_split_train_test(digits_data, digits_targets, 0.4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "batch_size  = 10\n",
    "dtrn = mnist_minibatch(xtrn[:, :, :, 1:num_samples], \n",
    "                       ytrn[1:num_samples], batch_size)\n",
    "dtst = mnist_minibatch(xtst[:, :, :, 1:num_samples], \n",
    "                       ytst[1:num_samples], batch_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the training data after grouping it into mini-batches, along with the associated \"heat-vectors\" used to encode the target values - note how there is a 1 in the index corresponding to the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  -->  5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAJiS0dEAP+Hj8y/AAABpUlEQVRo3u3ZP2sUQRzG8c9JDiyEgBYpLVQMgkXALnY2Ypo0gm/AYGMZEquDEMQEUlmk8w0IgoJFSBrFSgshECJiYZM0wX9VsNCzmBXCFmEWg8wNvy8sd8zt3pfnHmZv5o4gCIIgCILRp5d74iIeNs+/42dz8SNsYDfzfU7974T1C7M7/IYz+Nq6eIjHWC41Yf3CsZyTzmMcB5gYtYT1C7M6vDHKCUMYwvKFWfOw1xx9TLde28PnkhPWL8zqcNgc43h9ZLyHL1jFGn6XmLB+YVaHbXbwCbM4K+0vLmGuxIT1C7P2FlfxSir8Ll7gEOewhHv4hRlslpawfmH2/vCiNOfetsb72MJ1PJE6Liph/cLsDo9jHiuZCer/SEfj+7DNR2nNU2TCEIawfOGJzMO/vCsxYf3CE+nwVvP4rMSE9Qs7rWkuSOvTjSNjs1J3H3ClxIT1CzvNwwGmpN9qDqV9xW3pv8RBqQnrF3bq8BouYxLvsY47eI6npSasX9jpXnoTL7GPH9K9c68Z3yk1Yf3CTh328QALOI03uI/tkhPWLwyCIAj+nT/b0TaTbIZaSgAAAABJRU5ErkJggg==",
      "text/plain": [
       "28×28 Array{Gray{Float32},2}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1 # batch number\n",
    "j = 2 # index within batch\n",
    "mnist_view(dtrn, i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parameters\n",
    "\n",
    "Each node in convolutional layer is associated with a 3D block (`height` x `width` x `channel`) in the input tensor. Moreover, the convolutional layer itself has multiple output channels. So the layer is parameterized by a 4 dimensional weight tensor, commonly called a *convolutional kernel*.\n",
    "\n",
    "The output is produced by sliding the kernel across the input image skipping locations according to a pre-defined *stride* (but we'll just assume that to be 1 in this tutorial). Let's initialize some such kernels from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initweights() = [ xavier(Float32,5,5,1,20),zeros(Float32,1,1,20,1),   # Convolutional layer 1\n",
    "                  xavier(Float32,5,5,20,50), zeros(Float32,1,1,50,1), # Convolutional layer 2\n",
    "                  xavier(Float32,500,800), zeros(Float32,500,1),      # Fully connected layer 3\n",
    "                  xavier(Float32,10,500), zeros(Float32,10,1) ]       # Final layer\n",
    "\n",
    "w = initweights();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional layer 1 is composed of 20 5x5 filters\n",
    "- The resulting filtered image size is `(28-5+1 , 28-5+1) = (24, 24)`.\n",
    "- These values are \"maxpooled\" to reduces this further to `(24/2, 24/2) = (12, 12)`\n",
    "- Convolutional layer 2 is composed of 50 `5x5` filters.\n",
    "- The filtering reduces the image size to `(12-5+1, 12-5+1) = (8, 8)`\n",
    "- Maxpooling reduces this further to `(8/2, 8/2) = (4, 4)`\n",
    "- The 4D output tensor of layer 2 is thus of shape `(batch_size=10, 50, 4, 4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of each kernel is ($k_x, k_y$, ch_input, ch_output), where $k_x, k_y$ are the kernel size (sliding window size), ch_input is the number of input channels (1 for gray scale, 3 for RGB), and ch_output is the number of output channels (or output filters). Basically, the number of output channels is really the number of filters we want to create. Each new filter is convolved with each input filter. For the first layer the input is the image $x$, therefore we convolve 20 filters with a single input image (for RGB, we convolve all three images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=1 size(w[i]) =(5, 5, 1, 20)\n",
      "i=2 size(w[i]) =(1, 1, 20, 1)\n",
      "i=3 size(w[i]) =(5, 5, 20, 50)\n",
      "i=4 size(w[i]) =(1, 1, 50, 1)\n",
      "i=5 size(w[i]) =(500, 800)\n",
      "i=6 size(w[i]) =(500, 1)\n",
      "i=7 size(w[i]) =(10, 500)\n",
      "i=8 size(w[i]) =(10, 1)\n"
     ]
    }
   ],
   "source": [
    "for i=1:length(w)\n",
    "    println(\"i=\", i, \" size(w[i]) =\", size(w[i]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "We will build our `nnPredict()` function by introducing two new functions. First, we will new Knet's function [conv4](https://knet.readthedocs.io/en/latest/cnn.html#convolution) to execute convolutions or cross-correlations using filters specified with `w` over 4D tensors `x`. \n",
    "\n",
    "After that we will use Knet's function [pool](https://knet.readthedocs.io/en/latest/cnn.html#pooling). Pooling gives us a way to downsample in the spatial dimensions. Early convnets typically used average pooling, but max pooling tends to give better results. \n",
    "\n",
    "A typical pooling layer is defined by the `window size` (typically 2x2), `stride size` (typically (2,2)), and `pooling type`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnPredict (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nnPredict(w,x0)\n",
    "    x1 = pool(relu.(conv4(w[1], x0) .+ w[2]))\n",
    "    x2 = pool(relu.(conv4(w[3], x1) .+ w[4]))\n",
    "    x3 = relu.(w[5]*mat(x2) .+ w[6])\n",
    "    return w[7]*x3 .+ w[8]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this convolutional model, since we are again predicting from among 10 classes, we will use a cross-entropy loss function adapted from the Knet library, [Knet.nll](https://denizyuret.github.io/Knet.jl/latest/reference.html#Knet.nll)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nnLoss(w,x,ygold)\n",
    "    ypred = nnPredict(w,x)\n",
    "    ynorm = ypred .- log.(sum(exp.(ypred),1))\n",
    "    return -sum(ygold .* ynorm) / size(ygold,2)\n",
    "end\n",
    "\n",
    "lossgradient = Knet.grad(nnLoss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "\n",
    "For a default of 10 epochs (i.e., loops through the entire training dataset `dtrn`), update the weights based on the gradient computed from the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(w, dtrn, optim; epochs=10)\n",
    "    for epoch = 1:epochs\n",
    "        for (x, y) in dtrn\n",
    "            g = lossgradient(w,x,y)\n",
    "            update!(w,g,optim)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer (stochastic gradient descent) and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report (generic function with 2 methods)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function is the same as in Lecture 9\n",
    "function nnAccuracy(w, dtst, pred=nnPredict)\n",
    "    ncorrect = ninstance = nloss = 0.0\n",
    "    for (x, ygold) in dtst\n",
    "        ypred = pred(w, x)\n",
    "        ynorm = ypred .- log.(sum(exp.(ypred),1))\n",
    "        nloss += -sum(ygold .* ynorm)\n",
    "        ncorrect += sum(ygold .* (ypred .== maximum(ypred,1)))\n",
    "        ninstance += size(ygold,2)\n",
    "    end\n",
    "    return (ncorrect/ninstance, nloss/ninstance)\n",
    "end\n",
    "\n",
    "# report progress of training, same as Lecture 9\n",
    "function report(epoch, w, dtrn, dtst)\n",
    "    println((:epoch, epoch, :trn, nnAccuracy(w, dtrn), :tst, nnAccuracy(w, dtst)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 1, :trn, (0.786, 0.6303167221546173), :tst, (0.738, 0.7358147487640381))\n",
      "(:epoch, 2, :trn, (0.915, 0.2426262273788452), :tst, (0.86, 0.42318994545936583))\n",
      "(:epoch, 3, :trn, (0.954, 0.15300455713272096), :tst, (0.892, 0.34894015860557553))\n",
      "(:epoch, 4, :trn, (0.966, 0.10782810378074646), :tst, (0.907, 0.35852358627319336))\n",
      "(:epoch, 5, :trn, (0.988, 0.044941128730773924), :tst, (0.922, 0.31226648831367493))\n",
      "(:epoch, 6, :trn, (0.992, 0.03384730553627014), :tst, (0.937, 0.30676495814323423))\n",
      "(:epoch, 7, :trn, (0.997, 0.010302583694458007), :tst, (0.936, 0.2873646838665009))\n",
      "(:epoch, 8, :trn, (1.0, 0.0034970722198486327), :tst, (0.934, 0.2910602631568909))\n",
      "(:epoch, 9, :trn, (0.998, 0.006779372692108154), :tst, (0.941, 0.2950070059299469))\n",
      "(:epoch, 10, :trn, (1.0, 0.0013216438293457032), :tst, (0.941, 0.2811251482963562))\n"
     ]
    }
   ],
   "source": [
    "w = initweights();\n",
    "opt = optimizers(w, Sgd;  lr=0.1) # learning rate set to 0.1, try changing this\n",
    "\n",
    "for epoch = 1:10\n",
    "    train(w, dtrn, opt; epochs=1)\n",
    "    report(epoch, w, dtrn, dtst)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! consider that we only used 1000 samples, and this time around we were able to achieve around 94% accuracy on the `test` dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking beneath the surface\n",
    "\n",
    "The direct result of a prediction on a minibatch is a $10\\times10$ array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Float32,2}:\n",
       " -4.44269  17.2795      3.54485   …   3.79491   -3.86585   -6.10273 \n",
       " -3.74689   0.575148   31.7641       -0.790264   2.49911   -5.41517 \n",
       " 12.0134    2.32502     7.05492       1.11938   -2.29322   -7.58509 \n",
       " -2.42487  -6.19523   -15.3763       -0.826069  -3.38316    7.14428 \n",
       "  2.71294  -7.07945    -9.92849      -1.71197   -3.79654    9.11439 \n",
       " -8.67669  -5.66194   -16.5559    …  -2.87709   -6.11647   20.7778  \n",
       " -1.86479   2.47241     1.41845      -4.77856   -3.67178  -14.9172  \n",
       "  2.0547    1.25009    -1.2759       13.1416    13.0153     2.94635 \n",
       "  6.3246   -2.01893    -7.20068      -1.50822    3.97759   -4.66776 \n",
       " -2.76917  -3.62953    -0.372132     -3.89761    1.94401   -0.674803"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = nnPredict(w, dtst[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be compared with the heat-vectors encoding the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Float32,2}:\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0\n",
       " 1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytarget = dtst[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare the indices of the largest values in each column between predicted and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Float32,2}:\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytarget .* (ypred .== maximum(ypred,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where there is a `1.0` in the column, we got the correct prediction. This means the number of correct predictions in this batch is just the sum of the matrix, here 9 out of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0f0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncorrect = sum(ytarget .* (ypred .== maximum(ypred,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error occured in column 1, let's look at that digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  -->  9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAJiS0dEAP+Hj8y/AAAB1ElEQVRo3u3ZO2gWQRiF4UcRuzTBIiC/10g0WsQUYmNjymCKgCDYaKGNrZ1iaWkjprZOQOy8IFamkDQpLBQRVBRFiCCohWi0+DYQQ352guBOhjmw7LCz7MvhMLdvqaqqqqqqqqra/NqykZcP4CzGMIphvMJJfMUXLLd8Y+v/dlg+MDnDadzCT9wT2e3HBbzDoMj3bm4OywduS3npIG5gFlfEmFvRNHY27cEcHZYPTMpwCrtw29/5wVXMNO2POTosH5g0lz4RY2wMP9b07cAnPO/T37nD8oFJ4xC+49c6z4eb+xvt+XXisHxgUoZLOIV9eLmm72Jzf5yrw/KBSRnOiwxP4/qq5+dxrmkv5uqwfGDSejiABziOBcyJM+ExfEMPh/AiR4flA5PPh3twU6x/v8U+55LIc0rNsENg8p7mtZhPN53DCqzA/IHJ47CfduN9c2XpsHzghmre62lZnO9Hc3VYgRWYPzBpLj0q9qKLffrv5+ywfGBrhkfwCB9wBs9W9Z0Q9bWZto906bB8YOt62MNDjIgz4TU8xWHcETXUvTk7LB+YtKeZEP+dVv4TvsUQtmNcep2tE4flA5P3pT1cFjXTIXzGpBiTWTssH1hVVVVV9e/6A/meRFOctud6AAAAAElFTkSuQmCC",
      "text/plain": [
       "28×28 Array{Gray{Float32},2}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_view(dtst, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the corresponding column of the prediction and sort it by the predicted signal strengthes, we see that this 9 is mistaken for a 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Tuple{Int64,Float32},1}:\n",
       " (3, 12.0134)  \n",
       " (9, 6.3246)   \n",
       " (5, 2.71294)  \n",
       " (8, 2.0547)   \n",
       " (7, -1.86479) \n",
       " (4, -2.42487) \n",
       " (10, -2.76917)\n",
       " (2, -3.74689) \n",
       " (1, -4.44269) \n",
       " (6, -8.67669) "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = sortperm(ypred[:, 1], rev=true)\n",
    "collect(zip(order, ypred[order, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Questions (not assessed)\n",
    "\n",
    "####  Multilayer Perceptron model from Lecture 9\n",
    "\n",
    "- What is the total number of pixels in each MNIST input image?\n",
    "   - How many total input values are there to the model? (Understand why.)\n",
    "   - How many total output values? (Understand why.)\n",
    "\n",
    "- How many total parameters are there to the model? (Note that the parameters object `w` is a two-element array including both the large *weight* matrix and the smaller *bias* vector, which together compose the parameters of the model.)\n",
    "\n",
    "- What is the number of input values to the input layer?\n",
    "\n",
    "- What is the number of output values to the input layer (and hence the number of inputs to the first hidden layer)?\n",
    "\n",
    "- What is the number of output values to the first hidden layer (and hence the number of input values to the second hidden layer)?\n",
    "\n",
    "- What is the number of output values to the second hidden layer?\n",
    "\n",
    "- How many total parameters are there, including the weight matrices and bias vectors for all layers? (Note that in this model the parameters array `w` has more than two elements. Understand why.)\n",
    "\n",
    "### Convolutional Neural Network model from Lecture 10\n",
    "\n",
    "For the model as defined by the `initweights()` function (see also the corresponding `nnPredict` function which shows how to get to a prediction given the weights and an input), determine:\n",
    "\n",
    "- How many weights are in the first convolutional layer? (Not including the bias vector)\n",
    "- How many weights are in the second convolutional layer? (Not including the bias vector)\n",
    "- How many weights are in the next fully-connected layer? (Not including the bias vector)\n",
    "- How many weights are in the final layer? (Not including the bias vector)\n",
    "- How many weights are in the entire set of parameters (including all bias vectors)?\n",
    "- Which two digits  are most likely to be misclassified by the trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Contained in this example are nearly all the important ideas you'll need to start attacking problems in computer vision. While state-of-the-art vision systems incorporate a few more bells and whistles, they're all built on this foundation. Believe it or not, if you knew just the content in this tutorial 5 years ago, you could probably have sold a startup to a Fortune 500 company for millions of dollars. Fortunately (or unfortunately?), the world has gotten marginally more sophisticated, so we'll have to come up with some more sophisticated tutorials to follow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.4",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
